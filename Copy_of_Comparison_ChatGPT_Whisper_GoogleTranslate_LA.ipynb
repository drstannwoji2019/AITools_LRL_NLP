{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNm2sup2grF3rLm4G2u3iSs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drstannwoji2019/AITools_LRL_NLP/blob/main/Copy_of_Comparison_ChatGPT_Whisper_GoogleTranslate_LA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEfRhUzZMb6x",
        "outputId": "089f7015-8cde-466b-c34e-63ba094bfd45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.10.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from python-docx) (4.9.2)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184505 sha256=2b3cb15fa635e1473914b9955d548d223ea0a48f0e42af2af7a49b9c0b419715\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/8b/7c/09ae60c42c7ba4ed2dddaf2b8b9186cb105255856d6ed3dba5\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from fasttext) (63.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from fasttext) (1.22.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp39-cp39-linux_x86_64.whl size=4395543 sha256=1fa6120dfa4f98c4ad260dba61369b8f74769c681746dd5c834d8b9477f6b17e\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/57/bc/1741406019061d5664914b070bd3e71f6244648732bc96109e\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.4\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install scipy\n",
        "!pip install scikit-learn\n",
        "!pip install python-docx\n",
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "import docx\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Use WMD model to compare the documents\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.similarities import WmdSimilarity\n"
      ],
      "metadata": {
        "id": "n7NTsjlYNGpF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Reference document\n",
        "def read_word_doc(filename):\n",
        "    doc = docx.Document(filename)\n",
        "    text = []\n",
        "    for para in doc.paragraphs:\n",
        "        text.append(para.text)\n",
        "    return '\\n'.join(text)\n",
        "\n",
        "filename = \"/content/Igbo_OrigSource.docx\"\n",
        "reference = read_word_doc(filename)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove verse numbers\n",
        "    text = ''.join(filter(lambda x: not x.isdigit(), text))\n",
        "    \n",
        "    # Remove punctuations\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    \n",
        "    # Convert to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    return text\n",
        "\n",
        "text = reference\n",
        "reference = preprocess_text(text)"
      ],
      "metadata": {
        "id": "1h6m-d8LZdli"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load first candidate - ChatGPT\n",
        "def read_word_doc(filename):\n",
        "    doc = docx.Document(filename)\n",
        "    text = []\n",
        "    for para in doc.paragraphs:\n",
        "        text.append(para.text)\n",
        "    return '\\n'.join(text)\n",
        "\n",
        "filename = \"/content/ChatGPT_Igbo_Matt2_1-23.docx\"\n",
        "candidate1 = read_word_doc(filename)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove verse numbers\n",
        "    text = ''.join(filter(lambda x: not x.isdigit(), text))\n",
        "    \n",
        "    # Remove punctuations\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    \n",
        "    # Convert to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    return text\n",
        "\n",
        "text = candidate1\n",
        "candidate1 = preprocess_text(text)"
      ],
      "metadata": {
        "id": "b8282ytOZ8AL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load second candidate - GoogleTranslate\n",
        "def read_word_doc(filename):\n",
        "    doc = docx.Document(filename)\n",
        "    text = []\n",
        "    for para in doc.paragraphs:\n",
        "        text.append(para.text)\n",
        "    return '\\n'.join(text)\n",
        "\n",
        "filename = \"/content/IgboGoogleTranslate_NKJV.docx\"\n",
        "candidate2 = read_word_doc(filename)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove verse numbers\n",
        "    text = ''.join(filter(lambda x: not x.isdigit(), text))\n",
        "    \n",
        "    # Remove punctuations\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    \n",
        "    # Convert to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    return text\n",
        "\n",
        "text = candidate2\n",
        "candidate2 = preprocess_text(text)"
      ],
      "metadata": {
        "id": "xc4_sSb6aOwS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load third candidate - Whisper\n",
        "def read_word_doc(filename):\n",
        "    doc = docx.Document(filename)\n",
        "    text = []\n",
        "    for para in doc.paragraphs:\n",
        "        text.append(para.text)\n",
        "    return '\\n'.join(text)\n",
        "\n",
        "#def read_txt_file(filename):\n",
        "    #with open(filename, 'r') as file:\n",
        "        #text = file.read().replace('\\n', ' ')\n",
        "    #return text\n",
        "\n",
        "filename = \"/content/Whisper_Matiu2.docx\"\n",
        "candidate3 = read_word_doc(filename)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove verse numbers\n",
        "    text = ''.join(filter(lambda x: not x.isdigit(), text))\n",
        "    \n",
        "    # Remove punctuations\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    \n",
        "    # Convert to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    return text\n",
        "\n",
        "text = candidate3\n",
        "candidate3 = preprocess_text(text)"
      ],
      "metadata": {
        "id": "-g6ahYs3ac6n"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare ChatGPT to the Reference\n",
        "def bleu_score(reference, candidate, n):\n",
        "    reference = [reference.split()]\n",
        "    candidate = candidate.split()\n",
        "    weights = (1/n,) * n\n",
        "    score = sentence_bleu(reference, candidate, weights=weights)\n",
        "    return score\n",
        "\n",
        "reference = reference\n",
        "candidate = candidate1\n",
        "n = 2\n",
        "score1 = bleu_score(reference, candidate1, n)\n",
        "print(\"BLEU Score 1:\", score1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dkXidr9aqA5",
        "outputId": "f4182781-8fb8-45e5-85c8-f3ccec17ecaf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score 1: 0.09399142401550711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare GoogleTranslate to Reference\n",
        "def bleu_score(reference, candidate, n):\n",
        "    reference = [reference.split()]\n",
        "    candidate = candidate.split()\n",
        "    weights = (1/n,) * n\n",
        "    score = sentence_bleu(reference, candidate, weights=weights)\n",
        "    return score\n",
        "\n",
        "reference = reference\n",
        "candidate = candidate2\n",
        "n = 2\n",
        "score2 = bleu_score(reference, candidate2, n)\n",
        "print(\"BLEU Score 2:\", score2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n-xCJAKbHKK",
        "outputId": "15c3c6e1-1c98-46ff-d8ea-cfa98fdb0017"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score 2: 0.48373809611537255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Whisper to Reference\n",
        "def bleu_score(reference, candidate, n):\n",
        "    reference = [reference.split()]\n",
        "    candidate = candidate.split()\n",
        "    weights = (1/n,) * n\n",
        "    score = sentence_bleu(reference, candidate, weights=weights)\n",
        "    return score\n",
        "\n",
        "reference = reference\n",
        "candidate = candidate3\n",
        "n = 2\n",
        "score3 = bleu_score(reference, candidate3, n)\n",
        "print(\"BLEU Score 3:\", score3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOEKmA25bhwx",
        "outputId": "ae74db28-9b3a-485a-f5ad-a1313700cb22"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score 3: 0.07603997570895804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Whisper to GoogleTranslate\n",
        "def bleu_score(reference, candidate, n):\n",
        "    reference = [reference.split()]\n",
        "    candidate = candidate.split()\n",
        "    weights = (1/n,) * n\n",
        "    score = sentence_bleu(reference, candidate, weights=weights)\n",
        "    return score\n",
        "\n",
        "reference = candidate2\n",
        "candidate4 = candidate3\n",
        "n = 2\n",
        "score4 = bleu_score(reference, candidate4, n)\n",
        "print(\"BLEU Score 4:\", score4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rupRr2rDb0dB",
        "outputId": "84b7c61f-838a-48d2-b982-960a7fd0eda8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score 4: 0.05028157379785203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity between ChatGPT and reference\n",
        "def cosine_sim(text1, text2):\n",
        "    text1 = preprocess_text(text1)\n",
        "    text2 = preprocess_text(text2)\n",
        "    \n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vec1 = vectorizer.fit_transform([text1]).toarray()\n",
        "    vec2 = vectorizer.transform([text2]).toarray()\n",
        "    similarity = cosine_similarity(vec1, vec2)\n",
        "    return similarity[0][0]\n",
        "\n",
        "text1 = reference\n",
        "text2 = candidate1\n",
        "similarity1 = cosine_sim(text1, text2)\n",
        "print(\"Cosine Similarity:\", similarity1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21qjCWmEcduZ",
        "outputId": "04876b46-c80b-408b-a386-943bee535b72"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.6094605659612955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity between GoogleTranslate and reference\n",
        "def cosine_sim(text1, text2):\n",
        "    text1 = preprocess_text(text1)\n",
        "    text2 = preprocess_text(text2)\n",
        "    \n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vec1 = vectorizer.fit_transform([text1]).toarray()\n",
        "    vec2 = vectorizer.transform([text2]).toarray()\n",
        "    similarity = cosine_similarity(vec1, vec2)\n",
        "    return similarity[0][0]\n",
        "\n",
        "text1 = reference\n",
        "text2 = candidate2\n",
        "similarity2 = cosine_sim(text1, text2)\n",
        "print(\"Cosine Similarity:\", similarity2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVk02umcc9A5",
        "outputId": "0f5e6be0-2968-44b4-978d-e855d0804ecc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 1.000000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity between Whisper and reference\n",
        "def cosine_sim(text1, text2):\n",
        "    text1 = preprocess_text(text1)\n",
        "    text2 = preprocess_text(text2)\n",
        "    \n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vec1 = vectorizer.fit_transform([text1]).toarray()\n",
        "    vec2 = vectorizer.transform([text2]).toarray()\n",
        "    similarity = cosine_similarity(vec1, vec2)\n",
        "    return similarity[0][0]\n",
        "\n",
        "text1 = reference\n",
        "text2 = candidate3\n",
        "similarity3 = cosine_sim(text1, text2)\n",
        "print(\"Cosine Similarity:\", similarity3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQZqTtMpdFv9",
        "outputId": "69dfe501-8a50-4e9f-afee-3e5495aab6a5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.568050992597023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(set1, set2):\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "    jaccard_sim = len(intersection) / len(union)\n",
        "    return round(jaccard_sim, 4) # round to 4 decimal points\n",
        "\n",
        "document1 = reference\n",
        "document2 = candidate1\n",
        "\n",
        "# Convert documents to sets of words\n",
        "set1 = set(document1.lower().split())\n",
        "set2 = set(document2.lower().split())\n",
        "\n",
        "# Compute Jaccard similarity\n",
        "jaccard_sim = jaccard_similarity(set1, set2)\n",
        "\n",
        "print(\"Jaccard similarity:\", jaccard_sim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ljWfkvLdbAB",
        "outputId": "a164b9ed-a873-4f1e-a6f3-5fe0513d0324"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard similarity: 0.135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(set1, set2):\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "    jaccard_sim = len(intersection) / len(union)\n",
        "    return round(jaccard_sim, 4) # round to 4 decimal points\n",
        "\n",
        "document1 = reference\n",
        "document3 = candidate2\n",
        "\n",
        "# Convert documents to sets of words\n",
        "set1 = set(document1.lower().split())\n",
        "set2 = set(document3.lower().split())\n",
        "\n",
        "# Compute Jaccard similarity\n",
        "jaccard_sim2 = jaccard_similarity(set1, set2)\n",
        "\n",
        "print(\"Jaccard similarity:\", jaccard_sim2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqYSlSopdzb3",
        "outputId": "6bea4582-9757-484c-c2ea-2fc388d5ac54"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard similarity: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(set1, set2):\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "    jaccard_sim = len(intersection) / len(union)\n",
        "    return round(jaccard_sim, 4) # round to 4 decimal points\n",
        "\n",
        "document1 = reference\n",
        "document4 = candidate3\n",
        "\n",
        "# Convert documents to sets of words\n",
        "set1 = set(document1.lower().split())\n",
        "set2 = set(document4.lower().split())\n",
        "\n",
        "# Compute Jaccard similarity\n",
        "jaccard_sim3 = jaccard_similarity(set1, set2)\n",
        "\n",
        "print(\"Jaccard similarity:\", jaccard_sim3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvgvr7PWd-zJ",
        "outputId": "736e2d15-2919-4598-8a2f-9dfd4d255800"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard similarity: 0.0787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare whisper and googletranslate outputs\n",
        "def jaccard_similarity(set1, set2):\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "    jaccard_sim = len(intersection) / len(union)\n",
        "    return round(jaccard_sim, 4) # round to 4 decimal points\n",
        "   \n",
        "\n",
        "document1 = candidate2\n",
        "document5 = candidate3\n",
        "\n",
        "# Convert documents to sets of words\n",
        "set1 = set(document1.lower().split())\n",
        "set2 = set(document5.lower().split())\n",
        "\n",
        "# Compute Jaccard similarity\n",
        "jaccard_sim4 = jaccard_similarity(set1, set2)\n",
        "\n",
        "print(\"Jaccard similarity:\", jaccard_sim4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rluGS6fqePzf",
        "outputId": "fb47185a-8d01-4cfb-8b1d-cddb37e6c35e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard similarity: 0.0787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "def jaccard_similarity(set1, set2):\n",
        "  intersection = set1.intersection(set2)\n",
        "  union = set1.union(set2)\n",
        "  jaccard_sim = len(intersection) / len(union)\n",
        "  return round(jaccard_sim, 2) \n",
        "# round to 2 decimal points and convert to percentage # Create a DataFrame to store the results\n",
        "df = pd.DataFrame({\n",
        "    'Language Tool': [\"ChatGPT\", \"Google Translate\", \"Whisper\"],\n",
        "    'Translated Document': [candidate1, candidate2, candidate3],\n",
        "    'Bleu Score': [score1, score2, score3],\n",
        "    'Cosine Similarity': [similarity1, similarity2, similarity3],\n",
        "    'Jaccard Similarity': [jaccard_sim, jaccard_sim2, jaccard_sim3],\n",
        "})\n",
        "# Set the index to the Document Pair column, add grid lines, and display the DataFrame\n",
        "df.set_index('Translated Document', inplace=False)\n",
        "df.style.set_properties(**{'text-align': 'center', 'border': '1px solid black'}).set_table_styles([{'selector': 'th', 'props': [('border', '1px solid black')]}])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "4Kntl38Isja2",
        "outputId": "697a4b5c-0c3c-49c2-e243-1a44a17bb948"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f03a0db35e0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_a452d th {\n",
              "  border: 1px solid black;\n",
              "}\n",
              "#T_a452d_row0_col0, #T_a452d_row0_col1, #T_a452d_row0_col2, #T_a452d_row0_col3, #T_a452d_row0_col4, #T_a452d_row1_col0, #T_a452d_row1_col1, #T_a452d_row1_col2, #T_a452d_row1_col3, #T_a452d_row1_col4, #T_a452d_row2_col0, #T_a452d_row2_col1, #T_a452d_row2_col2, #T_a452d_row2_col3, #T_a452d_row2_col4 {\n",
              "  text-align: center;\n",
              "  border: 1px solid black;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_a452d\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_a452d_level0_col0\" class=\"col_heading level0 col0\" >Language Tool</th>\n",
              "      <th id=\"T_a452d_level0_col1\" class=\"col_heading level0 col1\" >Translated Document</th>\n",
              "      <th id=\"T_a452d_level0_col2\" class=\"col_heading level0 col2\" >Bleu Score</th>\n",
              "      <th id=\"T_a452d_level0_col3\" class=\"col_heading level0 col3\" >Cosine Similarity</th>\n",
              "      <th id=\"T_a452d_level0_col4\" class=\"col_heading level0 col4\" >Jaccard Similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_a452d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_a452d_row0_col0\" class=\"data row0 col0\" >ChatGPT</td>\n",
              "      <td id=\"T_a452d_row0_col1\" class=\"data row0 col1\" >mgbe jisos gaenwu onwe ya na betlehem nasụsụ judea na ugwo nanya herod mgbe na aka nụlọ jerusalem naakpọ onye nwere mmadụ abịa mere nwoke ọzọ bịara anya ndị jew anyị gwara ihe ọma ya mgbe o gara a nile anyị naeme ka anyị chọrọ ya na eme ka herod jiri naegosi ọ bụla jerusalem ndị naechi nri na ọ bụla ndị ojiji e nwere ike nkejibe ya o nweghị onye mere nwoke ndị jew na e nyere ndị nwere isi nụlọ na ndị nwere otu ndị banyere okwu onye mere nwoke ndị jew bụkwa ihe gịnị mere ndị ojiji nkejibe e nweghị ndị bethlehem nasụsụ judea biko kwuru nihi na aha gụọ nke nabata obi bethlehem nasụsụ judea ị bụrụ na ọ bụrụ ya di mkpa nokpuru ndị ọrụ nke judea nihi na ndị ofuma bịakwụ ka m gaa ya na o gaachọ ya nahịa ndị jew na eme ka herod jiri naekwu o naechi o chọrọ ya nke a eme ka a naagafee ya ka anyị kwuru ya naanyị chọrọ ya nime ihe ị chọrọ m o nweghị ndị ojiji e jiri naagụ ya wụnye na bethlehem naeso kwa nihi na e weta ya ka m naaga ya ka anyị chọrọ ya nke a na e nye ya naemepụta ya ya nwere ike gaa ihe ọzọ o nyere ha ka ha agaa nime ndị ojiji e jiri naachọ ya na eme ka ha naagụ ya ha gbachiara ha nwetara ya naeme ka maria anya na ha gọzi ya ha nwere ike gara ha ntị na ha nye ya ihe ndị naachọ ya gold frankincense na myrrh na eme ka ha chọrọ ya na ndị ojiji e jiri naegosi ya ha choro ha nokpuru ụmụaka ha ụzọ nọnụ aha ya naaga joseph na ndị ojiji chịrị ka gaa gbaa ụmụaka na ya na nne ya bịa iwu egypt wụrụ otu ọzọ biko ka herod naeme ya ike nwee ya nke a joseph kwuru ya jiri gaa na ya na nne ya wụrụ otu ọzọ naha ya chọrọ ya o nwetara ya naeme ka egypt ma ọ bụa naha ya chọrọ ya herod nwere ike nwee ya ọ bụghịa o nweghị otu obi ya mere ya naakpọ gbaa ụmụaka na ya na nne ya bịa ka anyị nwee ya na eme ka o nwere ike jiri nobi ya ya o nyere ndị mmadụ abụọ na jerusalem na okporo ndị asị ya ndị ahụ haa mmadụ bụrụ na herod bụ onye nwere ike ịkpọrọ ya ebe a wụrụ otu obi ya mere ya ma ọ bụa o nwere ike nwee ụmụaka na ya na nne ya na eme ka nke a nwere ahu na ndị mmadụ abụọ na jerusalem na ndị ọrụ ndị ojoo na judea ị kwuru ya nọnụ aha ya gaa wụrụ otu ọzọ biko na eme ka joseph jiri nọnụ aha ya o naaga egypt ma ọ bụa herod naagụ ya na ike gaa ya ọ bụghịa o nweghị obi ya mere ya ma ọ bụa joseph kwuru ya wụrụ otu ọzọ naaga na asụsụ galilee nụbọchị a e nyere ya naeme ka a gaagba ya biko nụlọ gị gịnị mere nke nabata obi nke nabatara onwe gị</td>\n",
              "      <td id=\"T_a452d_row0_col2\" class=\"data row0 col2\" >0.093991</td>\n",
              "      <td id=\"T_a452d_row0_col3\" class=\"data row0 col3\" >0.609461</td>\n",
              "      <td id=\"T_a452d_row0_col4\" class=\"data row0 col4\" >0.135000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a452d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_a452d_row1_col0\" class=\"data row1 col0\" >Google Translate</td>\n",
              "      <td id=\"T_a452d_row1_col1\" class=\"data row1 col1\" >ma mb͕e amusiri jisus na betlehem nke judia nubọchi herod bú eze le ndiamamihe si nọwuwaanyanwu bia jerusalem si òle ebe onye ahu amuru eze ndiju nọ nihi na ayi ahuwo kpakpandoya nòwuwaanyanwu we bia ikpọ isi ala nye ya mb͕e herọd bú eze nuru nka obi we di ya miri ya na jerusalem nile o we chikọta ndiisinchuàjà na ndiodeakwukwọ nile nke ndiju o we jua ha ase ebe agāmu kraist ya mere ha siri ya na betlehem nke judia nihi na otú a ka edeworo ya nọnu onyeamuma ma gi onwegi betlehem nke di nala juda ọ bughi onye kacha nta netiti ndiisi juda n’ihi na n’ime gị ka otu onye ọchịchị gaesi pụta bụ́ onye gaazụ ndị m izrel mb͕e ahu herọd mb͕e ọ kpọrọ ndiamamihe ahu na nzuzo o we si nọnuha kwuputa oge kpakpando ahu putara o we ziga ha na betlehem si jenu chọ nwantakiri ahu nkeọma mb͕e unu gāchọta kwa ya weghachirim okwu ka mwe bia kwa kpọ isi ala nye ya mb͕e ha nuru eze ha we pua ma le kpakpando nke ha huru nọwuwaanyanwu nēje niru ha rue mb͕e ọ biara guzoro nelu ebe nwatakiri ahu nọ mgbe ha hụrụ kpakpando ahụ ha ṅụrịrị ọṅụ nke ukwuu ma mb͕e ha batara nulo ha hu nwatakiri ahu na meri nneya ha we da nala kpọ isi ala nye ya ma mb͕e ha meghere àkù nile ha ha we nye ya onyinyeinatairuọma ọlaedo frankincense na myrrh mgbe ahụ ebe chineke dọrọ ha aka ná ntị ná nrọ ka ha ghara ịlaghachikwuru herọd ha si ụzọ ọzọ gawa obodo ha ma mb͕e ha lasiri le mọozi nke jehova mere ka ọ hu josef anya na nrọ si bilie kuru nwantakiri ahu na nneya b͕alaga nijipt nọdu nebe ahu rue mb͕e mgākpọ gi n’ihi na herọd gaachọ nwatakịrị ahụ ibibi ya mgbe o biliri o kpọrọ nwatakiri ahu na nneya n’abali je ijipt nọ kwa n’ebe ahu rue ọnwu herọd ka o we mezu ihe onyenweayi kwuru site n’ọnu onyeamuma si n’ijipt ka m’siworo mezu akpọrọ ọkpara m herọd mb͕e ọ huru na ndiamamihe ghọb͕uru ya iweya di ọku nkeuku o we ziga b͕ue umundikom nile ndi nọ na betlehem na nalaya nile site nonye b͕ara arọ abua we rue nokpuru dika mb͕e o kpebisiri ike naka ndiamamihe mgbe ahụ e mezuru ihe e kwuru site n’ọnụ jeremaya onye amụma naasị a nụrụ olu na rema ịkwa ákwá arịrị na oké iru újú rechel naakwakwara ụmụ ya ákwá ọ jụkwara ịnaakasi obi n’ihi na ha adịkwaghị ma mb͕e herod nwuru le mọozi nke onyenweayi biakutere josef na nrọ na ijipt si bilie kuru nwatakiri ahu na nneya je nala israel nihi ndi nāchọ nwantakiri ahu ndụ anwụọla o we bilie kuru nwatakiri ahu na nneya bata nala israel ma mgbe ọ nụrụ na akeleọs naachị judia nọnọdụ nna ya herọd ọ tụrụ egwu ịga ebe ahụ ma ebe chineke dọrọ ya aka ná ntị na nrọ ọ wezugara onwe ya ba nógbè galili o we bia biri nobodo anākpọ nazaret ka ihe ekwuru site nọnu ndiamuma we mezu si agēme ya onye nazaret</td>\n",
              "      <td id=\"T_a452d_row1_col2\" class=\"data row1 col2\" >0.483738</td>\n",
              "      <td id=\"T_a452d_row1_col3\" class=\"data row1 col3\" >1.000000</td>\n",
              "      <td id=\"T_a452d_row1_col4\" class=\"data row1 col4\" >1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a452d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_a452d_row2_col0\" class=\"data row2 col0\" >Whisper</td>\n",
              "      <td id=\"T_a452d_row2_col1\" class=\"data row2 col1\" >mba muru jizos nabet lehem kiju diya nuboti heru odubweze le ufodundi ma ri hen ndi sitere no uwa nyan ubyaruru jerusalem si kole e bono bonya muri ibuweze ndi ju ni hina nyi huru papan doya no uwa nyan u a nyi we ebya ipo isi ala nyeya mambu heru odubweze nuru ubi loroyan muri yana jerusalem nile uwe me kandi isi nchi waja nandi oda kuko nile nkendi juzu ko uwe joanonu ha eba gamu christi nke ha hawesia nabet lehem kiju diya ni hinu tua ke devoroyana kuko sitena ko nyan mumase gi bet lehem kwa ala juda e digin kenta kachasea neti tindi si juda moli ni hina neti tigi ko onyi si ga puta onyi ga zundin ke mizrael di katoru mba ho herod borohanan juzu bondia ho ma ri hen uwe jutan ko manonu ha ogi babandahun kena puti he hawesia haje bet lehem se gano chopo tan koma oku banyere wata kiri aho mambo bolono gachotaya biakorom kamo ngan uweje kwa bo isi alanya ya mbehanu ro kweza ho hawega ma leba bandaho nke ahoronu owanyamu ona ganiru ha re mbo dira guzo nelu ebe wata kiri ahono mbehanu ro babandaho hawe nguria ukongu nkuku hawe banyemu lo hon wata kiri aho yana meri neya hawe dana ala o isi ala nyeya hawe buge iba kwa ha che onye nye niru ya oledu na frankincense na meri ebe chineke duru hodo nanro kahare laga chijekuru herod hawe wezugo uweha ga ala hasitenu zozo mambeha wezuga siro uweha le mozi nkonyo wanyu uwero muyagosi joseph nanrosi bile kuru wata kiri aho nan neya balaga nijip no neba huru e mbobu lamgabwagi ni hina herod gajitongu wata kiri aho kolayani owe bile kuru wata kiri aho nan neya nabali wezugo uweha ga ejipt owe no neba huru e ongu herod koku wanyu wanyu kuru sitenonu onyamu maya we mezozi sitena ejipt kampopu tarangwamu mba aho herod mba aho nande aho nkemari hemeroyani hochi ono made yano binkuku owe ziga buchasia omakandi ikom nile dinabelehem nanokalaya nile sitenande barara abuwa we rida dikoga hunko jutarankoma nononde ahomari he mba aho keme zuro kwekuru sitenonu jeremaya onyamu masi anurolu narema ekwa akwa niru juku rachel nakwaro moyakwa otoge kwa kakasi e yobi ni hina hadegi mba ahero do ngoseri le mozi nkonyewa e wero nguyagosi joseph nanroni ejipt se bilie kurunga takiri ahonanea gaala israel ni hina ndenachon dunga takiri ahanguwo owe bilie kurunga takiri ahonanea gaala israel mambonro na akelios bweze judia nonodon naya herod oto regu ije e baho mba ebetine keduru yodon anro owe zigaro mweyagana koko galilee we biabirinu obodana o nazaret kukwekuru sitenonu ndiamu mawemezwe naga oyonye nazaret</td>\n",
              "      <td id=\"T_a452d_row2_col2\" class=\"data row2 col2\" >0.076040</td>\n",
              "      <td id=\"T_a452d_row2_col3\" class=\"data row2 col3\" >0.568051</td>\n",
              "      <td id=\"T_a452d_row2_col4\" class=\"data row2 col4\" >0.078700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}