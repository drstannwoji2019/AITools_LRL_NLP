{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyDtacpnuYY2MRMINvkk5r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drstannwoji2019/AITools_LRL_NLP/blob/main/Comparison_ChatGPT_Whisper_GoogleTranslate_LA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEfRhUzZMb6x",
        "outputId": "4afe5e27-983d-4f8a-ca8b-8da8bb09ac9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from python-docx) (4.9.2)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184505 sha256=fb0746ea53912148a949e5d9f240bf259dff5bec99377e23f849198b5fa99203\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/8b/7c/09ae60c42c7ba4ed2dddaf2b8b9186cb105255856d6ed3dba5\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from fasttext) (63.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from fasttext) (1.22.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp39-cp39-linux_x86_64.whl size=4395481 sha256=990078c704b5b3e112afd7444cd7449e18cab70d7a5d3f0be56e65c5acb5f1e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/57/bc/1741406019061d5664914b070bd3e71f6244648732bc96109e\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.4\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy\n",
        "!pip install scipy\n",
        "!pip install scikit-learn\n",
        "!pip install python-docx\n",
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "import docx\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Use WMD model to compare the documents\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.similarities import WmdSimilarity\n"
      ],
      "metadata": {
        "id": "n7NTsjlYNGpF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Reference document\n",
        "def read_word_doc(filename):\n",
        "    doc = docx.Document(filename)\n",
        "    text = []\n",
        "    for para in doc.paragraphs:\n",
        "        text.append(para.text)\n",
        "    return '\\n'.join(text)\n",
        "\n",
        "filename = \"/content/Igbo_OrigSource.docx\"\n",
        "reference = read_word_doc(filename)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove verse numbers\n",
        "    text = ''.join(filter(lambda x: not x.isdigit(), text))\n",
        "    \n",
        "    # Remove punctuations\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    \n",
        "    # Convert to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    return text\n",
        "\n",
        "text = reference\n",
        "reference = preprocess_text(text)"
      ],
      "metadata": {
        "id": "1h6m-d8LZdli"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load first candidate - ChatGPT\n",
        "def read_word_doc(filename):\n",
        "    doc = docx.Document(filename)\n",
        "    text = []\n",
        "    for para in doc.paragraphs:\n",
        "        text.append(para.text)\n",
        "    return '\\n'.join(text)\n",
        "\n",
        "filename = \"/content/ChatGPT_Igbo_Matt2_1-23.docx\"\n",
        "candidate1 = read_word_doc(filename)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove verse numbers\n",
        "    text = ''.join(filter(lambda x: not x.isdigit(), text))\n",
        "    \n",
        "    # Remove punctuations\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    \n",
        "    # Convert to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    return text\n",
        "\n",
        "text = candidate1\n",
        "candidate1 = preprocess_text(text)"
      ],
      "metadata": {
        "id": "b8282ytOZ8AL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load second candidate - GoogleTranslate\n",
        "def read_word_doc(filename):\n",
        "    doc = docx.Document(filename)\n",
        "    text = []\n",
        "    for para in doc.paragraphs:\n",
        "        text.append(para.text)\n",
        "    return '\\n'.join(text)\n",
        "\n",
        "filename = \"/content/IgboGoogleTranslate_NKJV.docx\"\n",
        "candidate2 = read_word_doc(filename)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove verse numbers\n",
        "    text = ''.join(filter(lambda x: not x.isdigit(), text))\n",
        "    \n",
        "    # Remove punctuations\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    \n",
        "    # Convert to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    return text\n",
        "\n",
        "text = candidate2\n",
        "candidate2 = preprocess_text(text)"
      ],
      "metadata": {
        "id": "xc4_sSb6aOwS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load third candidate - Whisper\n",
        "def read_word_doc(filename):\n",
        "    doc = docx.Document(filename)\n",
        "    text = []\n",
        "    for para in doc.paragraphs:\n",
        "        text.append(para.text)\n",
        "    return '\\n'.join(text)\n",
        "\n",
        "#def read_txt_file(filename):\n",
        "    #with open(filename, 'r') as file:\n",
        "        #text = file.read().replace('\\n', ' ')\n",
        "    #return text\n",
        "\n",
        "filename = \"/content/Whisper_Matiu2.docx\"\n",
        "candidate3 = read_word_doc(filename)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove verse numbers\n",
        "    text = ''.join(filter(lambda x: not x.isdigit(), text))\n",
        "    \n",
        "    # Remove punctuations\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    \n",
        "    # Convert to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    return text\n",
        "\n",
        "text = candidate3\n",
        "candidate3 = preprocess_text(text)"
      ],
      "metadata": {
        "id": "-g6ahYs3ac6n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare ChatGPT to the Reference\n",
        "def bleu_score(reference, candidate, n):\n",
        "    reference = [reference.split()]\n",
        "    candidate = candidate.split()\n",
        "    weights = (1/n,) * n\n",
        "    score = sentence_bleu(reference, candidate, weights=weights)\n",
        "    return score\n",
        "\n",
        "reference = reference\n",
        "candidate = candidate1\n",
        "n = 2\n",
        "score1 = bleu_score(reference, candidate1, n)\n",
        "print(\"BLEU Score 1:\", score1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dkXidr9aqA5",
        "outputId": "70ec810a-551c-47ba-dafe-b90a2e110fa9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score 1: 0.09399142401550711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare GoogleTranslate to Reference\n",
        "def bleu_score(reference, candidate, n):\n",
        "    reference = [reference.split()]\n",
        "    candidate = candidate.split()\n",
        "    weights = (1/n,) * n\n",
        "    score = sentence_bleu(reference, candidate, weights=weights)\n",
        "    return score\n",
        "\n",
        "reference = reference\n",
        "candidate = candidate2\n",
        "n = 2\n",
        "score2 = bleu_score(reference, candidate2, n)\n",
        "print(\"BLEU Score 2:\", score2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n-xCJAKbHKK",
        "outputId": "c5bfaca0-895f-43ac-8fa5-ba0ebb05db38"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score 2: 0.48373809611537255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Whisper to Reference\n",
        "def bleu_score(reference, candidate, n):\n",
        "    reference = [reference.split()]\n",
        "    candidate = candidate.split()\n",
        "    weights = (1/n,) * n\n",
        "    score = sentence_bleu(reference, candidate, weights=weights)\n",
        "    return score\n",
        "\n",
        "reference = reference\n",
        "candidate = candidate3\n",
        "n = 2\n",
        "score3 = bleu_score(reference, candidate3, n)\n",
        "print(\"BLEU Score 3:\", score3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOEKmA25bhwx",
        "outputId": "0a43bcaf-7799-41d4-8728-e494c1e3a344"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score 3: 0.07603997570895804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Whisper to GoogleTranslate\n",
        "def bleu_score(reference, candidate, n):\n",
        "    reference = [reference.split()]\n",
        "    candidate = candidate.split()\n",
        "    weights = (1/n,) * n\n",
        "    score = sentence_bleu(reference, candidate, weights=weights)\n",
        "    return score\n",
        "\n",
        "reference = candidate2\n",
        "candidate4 = candidate3\n",
        "n = 2\n",
        "score4 = bleu_score(reference, candidate4, n)\n",
        "print(\"BLEU Score 4:\", score4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rupRr2rDb0dB",
        "outputId": "085d5170-5ba9-4272-d523-c834256e353b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score 4: 0.05028157379785203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity between ChatGPT and reference\n",
        "def cosine_sim(text1, text2):\n",
        "    text1 = preprocess_text(text1)\n",
        "    text2 = preprocess_text(text2)\n",
        "    \n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vec1 = vectorizer.fit_transform([text1]).toarray()\n",
        "    vec2 = vectorizer.transform([text2]).toarray()\n",
        "    similarity = cosine_similarity(vec1, vec2)\n",
        "    return similarity[0][0]\n",
        "\n",
        "text1 = reference\n",
        "text2 = candidate1\n",
        "similarity1 = cosine_sim(text1, text2)\n",
        "print(\"Cosine Similarity:\", similarity1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21qjCWmEcduZ",
        "outputId": "d49c92ff-3469-4c4e-8b0b-e064c74d7198"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.6094605659612955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity between GoogleTranslate and reference\n",
        "def cosine_sim(text1, text2):\n",
        "    text1 = preprocess_text(text1)\n",
        "    text2 = preprocess_text(text2)\n",
        "    \n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vec1 = vectorizer.fit_transform([text1]).toarray()\n",
        "    vec2 = vectorizer.transform([text2]).toarray()\n",
        "    similarity = cosine_similarity(vec1, vec2)\n",
        "    return similarity[0][0]\n",
        "\n",
        "text1 = reference\n",
        "text2 = candidate2\n",
        "similarity2 = cosine_sim(text1, text2)\n",
        "print(\"Cosine Similarity:\", similarity2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVk02umcc9A5",
        "outputId": "4fabe838-271a-4539-c678-f9a33e60f3d4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 1.000000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity between Whisper and reference\n",
        "def cosine_sim(text1, text2):\n",
        "    text1 = preprocess_text(text1)\n",
        "    text2 = preprocess_text(text2)\n",
        "    \n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vec1 = vectorizer.fit_transform([text1]).toarray()\n",
        "    vec2 = vectorizer.transform([text2]).toarray()\n",
        "    similarity = cosine_similarity(vec1, vec2)\n",
        "    return similarity[0][0]\n",
        "\n",
        "text1 = reference\n",
        "text2 = candidate3\n",
        "similarity3 = cosine_sim(text1, text2)\n",
        "print(\"Cosine Similarity:\", similarity3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQZqTtMpdFv9",
        "outputId": "65fce585-fab8-41b9-e003-b7bf678808d6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.568050992597023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(set1, set2):\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "    jaccard_sim = len(intersection) / len(union)\n",
        "    return round(jaccard_sim, 4) # round to 4 decimal points\n",
        "\n",
        "document1 = reference\n",
        "document2 = candidate1\n",
        "\n",
        "# Convert documents to sets of words\n",
        "set1 = set(document1.lower().split())\n",
        "set2 = set(document2.lower().split())\n",
        "\n",
        "# Compute Jaccard similarity\n",
        "jaccard_sim = jaccard_similarity(set1, set2)\n",
        "\n",
        "print(\"Jaccard similarity:\", jaccard_sim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ljWfkvLdbAB",
        "outputId": "c7425bf6-5141-41ff-f066-69689b85b408"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard similarity: 0.135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(set1, set2):\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "    jaccard_sim = len(intersection) / len(union)\n",
        "    return round(jaccard_sim, 4) # round to 4 decimal points\n",
        "\n",
        "document1 = reference\n",
        "document3 = candidate2\n",
        "\n",
        "# Convert documents to sets of words\n",
        "set1 = set(document1.lower().split())\n",
        "set2 = set(document3.lower().split())\n",
        "\n",
        "# Compute Jaccard similarity\n",
        "jaccard_sim2 = jaccard_similarity(set1, set2)\n",
        "\n",
        "print(\"Jaccard similarity:\", jaccard_sim2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqYSlSopdzb3",
        "outputId": "d1b568b0-bf4f-456b-926e-d84fda7e96fa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard similarity: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(set1, set2):\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "    jaccard_sim = len(intersection) / len(union)\n",
        "    return round(jaccard_sim, 4) # round to 4 decimal points\n",
        "\n",
        "document1 = reference\n",
        "document4 = candidate3\n",
        "\n",
        "# Convert documents to sets of words\n",
        "set1 = set(document1.lower().split())\n",
        "set2 = set(document4.lower().split())\n",
        "\n",
        "# Compute Jaccard similarity\n",
        "jaccard_sim3 = jaccard_similarity(set1, set2)\n",
        "\n",
        "print(\"Jaccard similarity:\", jaccard_sim3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvgvr7PWd-zJ",
        "outputId": "1c0fec95-0d3a-4ff2-a5b4-c15f2fbc7748"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard similarity: 0.0787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare whisper and googletranslate outputs\n",
        "def jaccard_similarity(set1, set2):\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "    jaccard_sim = len(intersection) / len(union)\n",
        "    return round(jaccard_sim, 4) # round to 4 decimal points\n",
        "   \n",
        "\n",
        "document1 = candidate2\n",
        "document5 = candidate3\n",
        "\n",
        "# Convert documents to sets of words\n",
        "set1 = set(document1.lower().split())\n",
        "set2 = set(document5.lower().split())\n",
        "\n",
        "# Compute Jaccard similarity\n",
        "jaccard_sim4 = jaccard_similarity(set1, set2)\n",
        "\n",
        "print(\"Jaccard similarity:\", jaccard_sim4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rluGS6fqePzf",
        "outputId": "dce5133a-268b-4a0f-b352-dcf3d66c1edc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard similarity: 0.0787\n"
          ]
        }
      ]
    }
  ]
}